# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PWsMwpqkxghAonqm9J1D2Px6XfSEhyHp

# Task
normalized it using z-normalizations

Here is all the data you need:
"simulated_health_wellness_data.csv"

## Data loading

### Subtask:
Load the "simulated_health_wellness_data.csv" file.

**Reasoning**:
Import pandas and load the CSV file into a DataFrame, then display the head and shape to verify.
"""

import pandas as pd

df = pd.read_csv("simulated_health_wellness_data.csv")
display(df.head())
display(df.shape)

"""## Data exploration

### Subtask:
Explore the loaded data to understand its structure, identify numerical features, and check for missing values.

**Reasoning**:
Explore the data by examining data types, checking for missing values, calculating descriptive statistics, and visualizing the distributions of numerical features.
"""

# Check data types and missing values
print(df.info())
print(df.isnull().sum())
print(df.isnull().sum() / len(df))

# Descriptive statistics and visualization
print(df.describe())
import matplotlib.pyplot as plt
df.hist(figsize=(10, 8), bins=20)
plt.tight_layout()
plt.show()

# Correlation analysis
corr_matrix = df.corr()
display(corr_matrix)
plt.figure(figsize=(8, 6))
import seaborn as sns
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix Heatmap")
plt.show()

"""## Data cleaning

### Subtask:
Data Cleaning: Handle Missing Values and Inconsistencies

**Reasoning**:
Re-examine the dataset for missing values encoded as empty strings or specific placeholder values. Check for and address data inconsistencies or errors, including outliers or unrealistic values in numerical features.
"""

# Check for missing values represented as empty strings or other placeholders.
for col in df.columns:
    if df[col].dtype == 'object':  # Check only object (string) columns
        print(f"Checking column '{col}' for empty strings or placeholders")
        # Check for empty strings:
        empty_string_count = df[col].str.strip().eq('').sum()
        if empty_string_count > 0:
            print(f"Found {empty_string_count} empty string values in column '{col}'")
            # Handle empty string: Replace with NaN or mode depending on the context.
            # Here we replace empty strings with NaN for demonstration
            df[col] = df[col].replace('', float('nan'))
        # Check for other placeholders, e.g., 'N/A', '?', etc.
        # ... you may add checks for different placeholders here
        # Example: placeholder_count = df[col].str.strip().eq('N/A').sum()
    elif pd.api.types.is_numeric_dtype(df[col]):
        print(f"Checking column '{col}' for outliers")
        # Check for unrealistic values, e.g., negative values for certain measures
        min_val = df[col].min()
        max_val = df[col].max()
        if min_val < 0 and col in ['Exercise_Time_Min', 'Healthy_Meals_Per_Day', 'Sleep_Hours_Per_Night', 'BMI']:
             print(f"Negative values found in {col}, setting negative values to 0")
             df.loc[df[col] < 0, col] = 0 # replace negative values with zero
        # Example: Check for outliers using IQR
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        if not outliers.empty:
            print(f"Outliers found in column '{col}':\n{outliers}")
            # Here, we could either replace with a suitable value (e.g. median) or remove the outliers
# Check for duplicates
print(f"Number of duplicated rows: {df.duplicated().sum()}")
if df.duplicated().sum() > 0:
  print("Dropping duplicated rows...")
  df.drop_duplicates(inplace=True)

display(df.head())

"""## Data preparation

### Subtask:
Select Numerical Features for Normalization

**Reasoning**:
Create a new DataFrame `df_numerical` containing only the numerical features from the original DataFrame `df`, then display the first few rows to verify.
"""

# Select numerical features for normalization
numerical_cols = ['Exercise_Time_Min', 'Healthy_Meals_Per_Day', 'Sleep_Hours_Per_Night', 'Stress_Level', 'BMI']
df_numerical = df[numerical_cols].copy()

# Verify the data types
print(df_numerical.dtypes)

# Display the first few rows of df_numerical
display(df_numerical.head())

"""## Data wrangling

### Subtask:
Apply z-score normalization to the numerical features.

**Reasoning**:
Apply z-score normalization to the numerical features using StandardScaler.
"""

from sklearn.preprocessing import StandardScaler

# Initialize a StandardScaler object
scaler = StandardScaler()

# Fit the scaler on the df_numerical DataFrame
scaler.fit(df_numerical)

# Transform the df_numerical DataFrame using the fitted scaler
df_numerical = pd.DataFrame(scaler.transform(df_numerical), columns=df_numerical.columns)

# Display the first few rows of the normalized DataFrame
display(df_numerical.head())

"""## Data exploration

### Subtask:
Re-examine the normalized data

**Reasoning**:
Calculate descriptive statistics for the normalized data and visualize the distributions using histograms.
"""

# Calculate descriptive statistics
print(df_numerical.describe())

# Generate histograms
import matplotlib.pyplot as plt
df_numerical.hist(figsize=(10, 8), bins=20)
plt.tight_layout()
plt.show()

"""## Summary:

### Q&A
No questions detected in the provided data analysis task.

### Data Analysis Key Findings
* The dataset, "simulated\_health\_wellness\_data.csv", contains 200 rows and 5 numerical columns ('Exercise\_Time\_Min', 'Healthy\_Meals\_Per\_Day', 'Sleep\_Hours\_Per\_Night', 'Stress\_Level', and 'BMI') without any missing values.
* Negative values were found in 'Exercise\_Time\_Min', 'Healthy\_Meals\_Per\_Day', 'Sleep\_Hours\_Per\_Night', and 'BMI' columns and were replaced with 0.
* Outliers were identified in the numerical features using the IQR method but not explicitly handled in the provided code.
* Z-score normalization was successfully applied to the numerical features using `StandardScaler`.  The means of the normalized features are close to zero, and the standard deviations are close to one.

### Insights or Next Steps
* Investigate and handle the identified outliers to improve the model's robustness.
* Explore relationships between the normalized features and any dependent variables to gain further insights into the dataset.

"""

# %% [markdown]
# ## Modeling
#
# ### Subtask:
# Perform K-Means Clustering
#
# %% [markdown]
# **Reasoning**:
# Apply K-Means clustering to the normalized numerical data. We will choose a suitable number of clusters (k) and fit the model.
#
# %%
from sklearn.cluster import KMeans

# Choose the number of clusters (you can experiment with different values of k)
k = 3

# Initialize the KMeans model
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10) # Setting n_init to suppress warning

# Fit the model to the normalized data
kmeans.fit(df_numerical)

# Get the cluster labels for each data point
cluster_labels = kmeans.labels_

# Add the cluster labels back to the original DataFrame
df['Cluster'] = cluster_labels

# Display the first few rows with the added cluster labels
display(df.head())

# %% [markdown]
# ## Model Evaluation
#
# ### Subtask:
# Evaluate the K-Means Clustering Model
#
# %% [markdown]
# **Reasoning**:
# Evaluate the quality of the clustering using metrics like the silhouette score.
#
# %%
from sklearn.metrics import silhouette_score

# Calculate the silhouette score
silhouette_avg = silhouette_score(df_numerical, cluster_labels)

# Print the silhouette score
print(f"Silhouette Score: {silhouette_avg}")

# %% [markdown]
# ## Data exploration
#
# ### Subtask:
# Visualize the clusters
#
# %% [markdown]
# **Reasoning**:
# Visualize the clusters to understand the grouping of data points.
#
# %%
import seaborn as sns
import matplotlib.pyplot as plt

# Add the cluster labels to the normalized DataFrame for plotting
df_numerical['Cluster'] = cluster_labels

# Visualize the clusters (e.g., using a scatter plot of two features)
# You might want to change the features to visualize different combinations
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_numerical, x='Exercise_Time_Min', y='Sleep_Hours_Per_Night', hue='Cluster', palette='viridis')
plt.title('K-Means Clustering')
plt.show()

# You can also visualize the distribution of features within each cluster using boxplots or violins plots
# For example, a boxplot of 'BMI' for each cluster:
plt.figure(figsize=(8, 6))
sns.boxplot(data=df, x='Cluster', y='BMI')
plt.title('BMI Distribution per Cluster')
plt.show()

# Drop the 'Cluster' column from df_numerical to avoid issues in subsequent steps if any
df_numerical = df_numerical.drop('Cluster', axis=1)

# %% [markdown]
# ## Model Evaluation Summary
#
# ### Subtask:
# Summarize the K-Means Clustering Model Evaluation
#
# %% [markdown]
# **Reasoning**:
# Create a summary table for the K-Means clustering evaluation metrics, including Inertia and Silhouette Score.
#
# %%
# Get the inertia (within-cluster sum of squares) from the fitted KMeans model
inertia = kmeans.inertia_

# Create a dictionary with evaluation metrics
evaluation_summary = {
    'Metric': ['Inertia', 'Silhouette Score'],
    'Value': [inertia, silhouette_avg]
}

# Create a pandas DataFrame from the dictionary
evaluation_df = pd.DataFrame(evaluation_summary)

# Display the evaluation summary table
display(evaluation_df)

# %% [markdown]
# ## Model Evaluation Summary
#
# ### Subtask:
# Summarize the K-Means clustering evaluation metrics, including Inertia and Silhouette Score.
#
# %% [markdown]
# **Reasoning**:
# Create a summary table for the K-Means clustering evaluation metrics, including Inertia and Silhouette Score.
#
# %%
# Get the inertia (within-cluster sum of squares) from the fitted KMeans model
inertia = kmeans.inertia_

# Create a dictionary with evaluation metrics
evaluation_summary = {
    'Metric': ['Inertia', 'Silhouette Score'],
    'Value': [inertia, silhouette_avg]
}

# Create a pandas DataFrame from the dictionary
evaluation_df = pd.DataFrame(evaluation_summary)

# Display the evaluation summary table
display(evaluation_df)

# %% [markdown]
# ## Data exploration (Tuned Model)
#
# ### Subtask:
# Visualize the clusters from the tuned K-Means model
#
# %% [markdown]
# **Reasoning**:
# Visualize the clusters with the chosen optimal k to understand the grouping of data points in the final model.
#
# %%
import seaborn as sns
import matplotlib.pyplot as plt

# Add the tuned cluster labels to the normalized DataFrame for plotting
# Create a copy to avoid modifying the original df_numerical
df_numerical_tuned_viz = df_numerical.copy()
df_numerical_tuned_viz['Cluster'] = cluster_labels_tuned

# Visualize the clusters using a scatter plot of two features
# You can change 'Exercise_Time_Min' and 'Sleep_Hours_Per_Night' to visualize different combinations
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_numerical_tuned_viz, x='Exercise_Time_Min', y='Sleep_Hours_Per_Night', hue='Cluster', palette='viridis')
plt.title(f'Tuned K-Means Clustering (k={optimal_k})')
plt.show()

# You can also visualize the distribution of features within each cluster using boxplots or violins plots
# For example, a boxplot of 'BMI' for each cluster:
plt.figure(figsize=(8, 6))
sns.boxplot(data=df, x='Cluster', y='BMI')
plt.title(f'BMI Distribution per Cluster (k={optimal_k})')
plt.show()

# Remove the temporary 'Cluster' column from the copied DataFrame to keep it clean
df_numerical_tuned_viz = df_numerical_tuned_viz.drop('Cluster', axis=1)

# %% [markdown]
# ## Data exploration (Agglomerative Clustering)
#
# ### Subtask:
# Visualize the clusters from the Agglomerative Hierarchical Clustering model.
#
# %% [markdown]
# **Reasoning**:
# Visualize the clusters to understand the grouping of data points.
#
# %%
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming you have run the Agglomerative Clustering code and have 'agg_cluster_labels' and 'n_clusters_agg'

# Add the agglomerative cluster labels to the normalized DataFrame for plotting
df_numerical_agg_viz = df_numerical.copy() # Create a copy
df_numerical_agg_viz['Agg_Cluster'] = agg_cluster_labels

# Visualize the clusters using a scatter plot of two features
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_numerical_agg_viz, x='Exercise_Time_Min', y='Sleep_Hours_Per_Night', hue='Agg_Cluster', palette='viridis')
plt.title(f'Agglomerative Hierarchical Clustering (n_clusters={n_clusters_agg})')
plt.show()

# You can also visualize the distribution of features within each cluster using boxplots or violins plots
plt.figure(figsize=(8, 6))
sns.boxplot(data=df, x='Agg_Cluster', y='BMI')
plt.title(f'BMI Distribution per Agglomerative Cluster (n_clusters={n_clusters_agg})')
plt.show()
# %% [markdown]
# ## Data exploration (Agglomerative Clustering)
#
# ### Subtask:
# Visualize the clusters from the Agglomerative Hierarchical Clustering model.
#
# %% [markdown]
# **Reasoning**:
# Visualize the clusters to understand the grouping of data points.
#
# %%
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming you have run the Agglomerative Clustering code and have 'agg_cluster_labels' and 'n_clusters_agg'

# Add the agglomerative cluster labels to the normalized DataFrame for plotting
df_numerical_agg_viz = df_numerical.copy() # Create a copy
df_numerical_agg_viz['Agg_Cluster'] = agg_cluster_labels

# Visualize the clusters using a scatter plot of two features
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_numerical_agg_viz, x='Exercise_Time_Min', y='Sleep_Hours_Per_Night', hue='Agg_Cluster', palette='viridis')
plt.title(f'Agglomerative Hierarchical Clustering (n_clusters={n_clusters_agg})')
plt.show()

# You can also visualize the distribution of features within each cluster using boxplots or violins plots
plt.figure(figsize=(8, 6))
sns.boxplot(data=df, x='Agg_Cluster', y='BMI')
plt.title(f'BMI Distribution per Agglomerative Cluster (n_clusters={n_clusters_agg})')
plt.show()

# Remove the temporary 'Agg_Cluster' column from the copied DataFrame
df_numerical_agg_viz = df_numerical_agg_viz.drop('Agg_Cluster', axis=1)

# Assuming you have performed Agglomerative Hierarchical Clustering and have:
# 1. Your normalized data: df_numerical
# 2. The cluster labels from Agglomerative Clustering: agg_cluster_labels

import numpy as np

# Calculate the pseudo-inertia for Agglomerative Clustering

# 1. Get the unique cluster labels
unique_clusters = np.unique(agg_cluster_labels)

# Initialize the pseudo-inertia
pseudo_inertia = 0

# Iterate through each cluster
for cluster_label in unique_clusters:
    # Get the data points belonging to the current cluster
    cluster_points = df_numerical[agg_cluster_labels == cluster_label]

    # Calculate the centroid (mean) of the current cluster
    cluster_centroid = np.mean(cluster_points, axis=0)

    # Calculate the sum of squared distances of points to the centroid for this cluster
    squared_distances = np.sum((cluster_points - cluster_centroid) ** 2)

    # Add the squared distances to the total pseudo-inertia
    pseudo_inertia += squared_distances

# Print the calculated pseudo-inertia
print(f"Pseudo-Inertia for Agglomerative Hierarchical Clustering (n_clusters={n_clusters_agg}): {pseudo_inertia}")

# %% [markdown]
# ## Model Evaluation Summary (Agglomerative Clustering with Pseudo-Inertia)
#
# ### Subtask:
# Summarize the Agglomerative Hierarchical Clustering evaluation metrics, including Pseudo-Inertia and Silhouette Score, in a table.
#
# %% [markdown]
# **Reasoning**:
# Create a summary table displaying the Pseudo-Inertia and Silhouette Score for the Agglomerative Clustering.
#
# %%
import pandas as pd

# Create a dictionary for the evaluation table
# Assuming you have calculated silhouette_avg_agg and n_clusters_agg

evaluation_data_agg_with_pseudo_inertia = {
    'Metric': ['Pseudo-Inertia', 'Silhouette Score'],
    'Value': [pseudo_inertia, silhouette_avg_agg]
}

# Create a pandas DataFrame from the dictionary
evaluation_df_agg_summary_with_pseudo_inertia = pd.DataFrame(evaluation_data_agg_with_pseudo_inertia)

# Display the evaluation table
print(f"Evaluation Summary for Agglomerative Clustering (n_clusters={n_clusters_agg}):")
display(evaluation_df_agg_summary_with_pseudo_inertia)

# %% [markdown]
# ## Tuned Agglomerative Hierarchical Clustering Results
#
# ### Subtask:
# Perform Agglomerative Hierarchical Clustering with the chosen optimal number of clusters and display the results.
#
# %% [markdown]
# **Reasoning**:
# Based on analyzing the dendrogram and potentially silhouette scores for different cluster numbers, we apply Agglomerative Clustering with the optimal number of clusters and examine the resulting cluster assignments.
#
# %%
from sklearn.cluster import AgglomerativeClustering

# Assuming you have determined an optimal_n_clusters_agg based on previous analysis (e.g., dendrogram)
# Replace 'optimal_n_clusters_agg' with the actual value you chose.
optimal_n_clusters_agg = 3  # Example: Let's assume you chose 3 clusters

# Initialize the AgglomerativeClustering model with the optimal number of clusters
agg_clustering_tuned = AgglomerativeClustering(n_clusters=optimal_n_clusters_agg)

# Fit the model to the normalized data and get the cluster labels
agg_cluster_labels_tuned = agg_clustering_tuned.fit_predict(df_numerical)

# Add the cluster labels back to the original DataFrame (using a new column name to distinguish from K-Means)
df['Agg_Cluster_Tuned'] = agg_cluster_labels_tuned

# Display the first few rows with the tuned agglomerative cluster labels
print(f"First 5 rows with Tuned Agglomerative Cluster labels (n_clusters={optimal_n_clusters_agg}):")
display(df.head())

# %% [markdown]
# ## Data exploration (Tuned Agglomerative Clustering)
#
# ### Subtask:
# Visualize the clusters from the tuned Agglomerative Hierarchical Clustering model.
#
# %% [markdown]
# **Reasoning**:
# Visualize the clusters with the chosen optimal n_clusters to understand the grouping of data points in the final agglomerative model.
#
# %%
import seaborn as sns
import matplotlib.pyplot as plt

# Add the tuned agglomerative cluster labels to the normalized DataFrame for plotting
df_numerical_agg_tuned_viz = df_numerical.copy() # Create a copy
df_numerical_agg_tuned_viz['Agg_Cluster_Tuned'] = agg_cluster_labels_tuned

# Visualize the clusters using a scatter plot of two features
# You can change 'Exercise_Time_Min' and 'Sleep_Hours_Per_Night' to visualize different combinations
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_numerical_agg_tuned_viz, x='Exercise_Time_Min', y='Sleep_Hours_Per_Night', hue='Agg_Cluster_Tuned', palette='viridis')
plt.title(f'Tuned Agglomerative Hierarchical Clustering (n_clusters={optimal_n_clusters_agg})')
plt.show()

# You can also visualize the distribution of features within each cluster using boxplots or violins plots
plt.figure(figsize=(8, 6))
sns.boxplot(data=df, x='Agg_Cluster_Tuned', y='BMI')
plt.title(f'BMI Distribution per Tuned Agglomerative Cluster (n_clusters={optimal_n_clusters_agg})')
plt.show()

# Remove the temporary 'Agg_Cluster_Tuned' column from the copied DataFrame
df_numerical_agg_tuned_viz = df_numerical_agg_tuned_viz.drop('Agg_Cluster_Tuned', axis=1)

# %% [markdown]
# ## Tuned Agglomerative Model Evaluation
#
# ### Subtask:
# Evaluate the Tuned Agglomerative Hierarchical Clustering Model.
#
# %% [markdown]
# **Reasoning**:
# Calculate and display evaluation metrics for the Agglomerative Clustering model with the chosen optimal n_clusters.
#
# %%
from sklearn.metrics import silhouette_score
import numpy as np # Needed for pseudo-inertia calculation

# Calculate the silhouette score for the tuned Agglomerative Clustering
silhouette_avg_agg_tuned = silhouette_score(df_numerical, agg_cluster_labels_tuned)

# Calculate the Pseudo-Inertia for the tuned Agglomerative Clustering (if you wish)
# Remember this is not the standard Inertia metric for Agglomerative Clustering
pseudo_inertia_agg_tuned = 0
unique_clusters_tuned = np.unique(agg_cluster_labels_tuned)
for cluster_label in unique_clusters_tuned:
    cluster_points_tuned = df_numerical[agg_cluster_labels_tuned == cluster_label]
    cluster_centroid_tuned = np.mean(cluster_points_tuned, axis=0)
    pseudo_inertia_agg_tuned += np.sum((cluster_points_tuned - cluster_centroid_tuned) ** 2)


# %% [markdown]
# ## Tuned Agglomerative Model Evaluation Summary
#
# ### Subtask:
# Summarize the Tuned Agglomerative Hierarchical Clustering evaluation metrics in a table.
#
# %% [markdown]
# **Reasoning**:
# Create a summary table displaying the evaluation metrics for the tuned Agglomerative Clustering model.
#
# %%
import pandas as pd

# Create a dictionary for the evaluation table
evaluation_data_agg_tuned = {
    'Metric': ['Silhouette Score', 'Pseudo-Inertia'], # Including Pseudo-Inertia as requested
    'Value': [silhouette_avg_agg_tuned, pseudo_inertia_agg_tuned]
}

# Create a pandas DataFrame from the dictionary
evaluation_df_agg_tuned_summary = pd.DataFrame(evaluation_data_agg_tuned)

# Display the evaluation table
print(f"Evaluation Summary for Tuned Agglomerative Clustering (n_clusters={optimal_n_clusters_agg}):")
display(evaluation_df_agg_tuned_summary)

# %% [markdown]
# ## Principal Component Analysis (PCA) and Visualization with Clusters
#
# ### Subtask:
# Perform PCA to reduce dimensionality to 2 components and visualize with cluster labels.
#
# %% [markdown]
# **Reasoning**:
# Apply PCA to the normalized data to capture the most variance in 2 dimensions, and then visualize these components, coloring points by their assigned cluster to see how clusters separate in the reduced space.
#
# %%
from sklearn.decomposition import PCA

# Initialize PCA to reduce to 2 components
pca = PCA(n_components=2)

# Fit PCA on the normalized data and transform it
principal_components = pca.fit_transform(df_numerical)

# Create a DataFrame for the principal components
principal_df = pd.DataFrame(data = principal_components, columns = ['principal component 1', 'principal component 2'])

# --- Add Cluster Labels ---
# Choose which set of cluster labels you want to visualize.
# You can use either the tuned K-Means labels or the tuned Agglomerative labels.
# Make sure the chosen cluster labels array/Series has the same index/order as df_numerical.

# Option 1: Use Tuned K-Means Cluster Labels (if you have run the tuned K-Means section)
# Assuming 'cluster_labels_tuned' holds the labels from your tuned K-Means model
if 'cluster_labels_tuned' in locals():
    principal_df['Cluster'] = cluster_labels_tuned
    clustering_model_name = f'K-Means (k={optimal_k})'
    cluster_column_name = 'Cluster' # The column name in principal_df

# Option 2: Use Tuned Agglomerative Cluster Labels (if you have run the tuned Agglomerative section)
# Assuming 'agg_cluster_labels_tuned' holds the labels from your tuned Agglomerative model
elif 'agg_cluster_labels_tuned' in locals():
    principal_df['Cluster'] = agg_cluster_labels_tuned
    clustering_model_name = f'Agglomerative (n_clusters={optimal_n_clusters_agg})'
    cluster_column_name = 'Cluster' # The column name in principal_df

# If neither tuned model has been run, you might want to use the initial clustering results
elif 'cluster_labels' in locals():
    principal_df['Cluster'] = cluster_labels # Using initial K-Means labels
    clustering_model_name = f'K-Means (k={k})'
    cluster_column_name = 'Cluster'
elif 'agg_cluster_labels' in locals():
     principal_df['Cluster'] = agg_cluster_labels # Using initial Agglomerative labels
     clustering_model_name = f'Agglomerative (n_clusters={n_clusters_agg})'
     cluster_column_name = 'Cluster'
else:
    print("No cluster labels found from either K-Means or Agglomerative Clustering.")
    clustering_model_name = 'No Clustering Performed'
    cluster_column_name = None # Indicate that no cluster column exists


# --- Visualize the Principal Components with Cluster Labels ---
if cluster_column_name: # Only visualize if cluster labels were added
    plt.figure(figsize=(10, 8))
    sns.scatterplot(x='principal component 1', y='principal component 2', hue=cluster_column_name, data=principal_df, palette='viridis', s=50)
    plt.title(f'First Two Principal Components with {clustering_model_name} Clusters')
    plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2f}% variance)')
    plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2f}% variance)')
    plt.grid(True)
    plt.show()

# %% [markdown]
# ## PCA Evaluation
#
# ### Subtask:
# Display the explained variance ratio for the principal components.
#
# %% [markdown]
# **Reasoning**:
# The explained variance ratio indicates how much of the original data's variance is captured by each principal component, which helps assess the effectiveness of the dimensionality reduction.
#
# %%
# Assuming you have run the PCA code and have the 'pca' object

# Display the explained variance ratio for each principal component
print("Explained Variance Ratio for each Principal Component:")
for i, ratio in enumerate(pca.explained_variance_ratio_):
    print(f"  Principal Component {i+1}: {ratio:.4f}")

# Display the total explained variance by the selected components
total_explained_variance = pca.explained_variance_ratio_.sum()
print(f"\nTotal Explained Variance by {pca.n_components_} Principal Components: {total_explained_variance:.4f}")

# %% [markdown]
# ## PCA Tuning (Selecting Number of Components)
#
# ### Subtask:
# Visualize the cumulative explained variance to help determine the number of principal components to retain.
#
# %% [markdown]
# **Reasoning**:
# Plotting the cumulative explained variance helps understand how much of the original data's variability is captured as more principal components are included, aiding in the decision of how many components to keep.
#
# %%
# Perform PCA on the full data to see explained variance for all components
pca_full = PCA()
pca_full.fit(df_numerical)

# Calculate the cumulative explained variance
cumulative_explained_variance = np.cumsum(pca_full.explained_variance_ratio_)

# Plot the cumulative explained variance
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')
plt.title('Cumulative Explained Variance by Number of Principal Components')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance Ratio')
plt.grid(True)
plt.xticks(range(1, len(cumulative_explained_variance) + 1))
plt.axhline(y=0.95, color='r', linestyle='-', label='95% Variance') # Example: Horizontal line at 95% variance
plt.legend()
plt.show()

# %% [markdown]
# ## Tuned PCA Results
#
# ### Subtask:
# Based on the cumulative explained variance, describe the chosen number of principal components and the variance explained.
#
# %% [markdown]
# **Reasoning**:
# Based on the visualization, we articulate the decision regarding the number of principal components and the percentage of variance captured by this reduced dimensionality.
#
# %%
# Based on the cumulative explained variance plot, choose a number of components.
# For this example, let's assume you decide to keep 2 components based on your previous analysis.
tuned_n_components = 2 # Replace with the number you choose based on the plot

# Get the explained variance ratio for the tuned number of components
explained_variance_tuned = pca_full.explained_variance_ratio_[:tuned_n_components].sum()

print(f"Tuned PCA: Retaining {tuned_n_components} principal components.")
print(f"Total variance explained by {tuned_n_components} components: {explained_variance_tuned:.4f}")

# %% [markdown]
# ## Tuned PCA Evaluation Summary
#
# ### Subtask:
# Summarize the evaluation of the tuned PCA.
#
# %% [markdown]
# **Reasoning**:
# Create a summary of the tuned PCA evaluation, focusing on the explained variance.
#
# %%
# Create a dictionary for the evaluation summary
pca_tuned_evaluation = {
    'Metric': ['Number of Components Retained', 'Total Explained Variance'],
    'Value': [tuned_n_components, explained_variance_tuned]
}

# Create a pandas DataFrame from the dictionary
pca_tuned_evaluation_df = pd.DataFrame(pca_tuned_evaluation)

# Display the evaluation summary
display(pca_tuned_evaluation_df)

# %% [markdown]
# ## Tuned PCA Results
#
# ### Subtask:
# Based on the cumulative explained variance, describe the chosen number of principal components and the variance explained.
#
# %% [markdown]
# **Reasoning**:
# Based on the visualization, we articulate the decision regarding the number of principal components and the percentage of variance captured by this reduced dimensionality.
#
# %%
# Based on the cumulative explained variance plot, choose a number of components.
# For this example, let's assume you decide to keep 2 components based on your previous analysis.
tuned_n_components = 2 # Replace with the number you choose based on the plot

# Assuming you have run PCA on the full data ('pca_full') to get explained variance for all components
# If not, you might need to re-run PCA with n_components=None or all features initially.

# Get the explained variance ratio for the tuned number of components
# We use the pca_full object fitted on all components to get the variance ratios
if 'pca_full' in locals():
    explained_variance_tuned = pca_full.explained_variance_ratio_[:tuned_n_components].sum()
else:
    print("Warning: 'pca_full' not found. Calculating explained variance for the previously used 'pca' object.")
    # If pca_full wasn't run, we can use the explained variance from the 'pca' object
    # assuming it was fitted with a sufficient number of components initially or n_components=None
    explained_variance_tuned = pca.explained_variance_ratio_.sum()
    # Note: This might not be the *cumulative* variance if pca was fitted with n_components < total features


print(f"Tuned PCA: Retaining {tuned_n_components} principal components.")
print(f"Total variance explained by {tuned_n_components} components: {explained_variance_tuned:.4f}")

# %% [markdown]
# ## Tuned PCA Evaluation Summary
#
# ### Subtask:
# Summarize the evaluation of the tuned PCA.
#
# %% [markdown]
# **Reasoning**:
# Create a summary of the tuned PCA evaluation, focusing on the explained variance.
#
# %%
import pandas as pd

# Create a dictionary for the evaluation summary
pca_tuned_evaluation = {
    'Metric': ['Number of Components Retained', 'Total Explained Variance'],
    'Value': [tuned_n_components, explained_variance_tuned]
}

# Create a pandas DataFrame from the dictionary
pca_tuned_evaluation_df = pd.DataFrame(pca_tuned_evaluation)

# Display the evaluation summary
display(pca_tuned_evaluation_df)

# %% [markdown]
# ## Tuned PCA Clustering Model Display
#
# ### Subtask:
# Visualize the clusters on the tuned PCA components.
#
# %% [markdown]
# **Reasoning**:
# Visualize the data points projected onto the tuned principal components, colored by their assigned cluster labels, to assess how well the clusters are separated in the reduced dimensional space.
#
# %%
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
import pandas as pd

# Assuming you have already performed:
# 1. Normalization of your numerical data into 'df_numerical'.
# 2. Tuned PCA to determine 'tuned_n_components' and have the 'pca_full' object fitted to all components
#    OR you have fitted a PCA model with 'n_components=tuned_n_components'.
# 3. Performed clustering (either K-Means or Agglomerative) and have the tuned cluster labels
#    available (e.g., 'cluster_labels_tuned' or 'agg_cluster_labels_tuned').

# Re-fit PCA with the tuned number of components to get the transformed data
# If pca_full was fitted on all components, you can use its components.
# If you only have a 'pca' object fitted with fewer components, re-fit it.
pca_tuned = PCA(n_components=tuned_n_components)
principal_components_tuned = pca_tuned.fit_transform(df_numerical)

# Create a DataFrame for the tuned principal components
principal_df_tuned = pd.DataFrame(data = principal_components_tuned,
                                  columns = [f'principal component {i+1}' for i in range(tuned_n_components)])

# --- Add Cluster Labels ---
# Choose which set of tuned cluster labels you want to visualize.
# Prioritize tuned models if available.

# Option 1: Use Tuned K-Means Cluster Labels
if 'cluster_labels_tuned' in locals():
    principal_df_tuned['Cluster'] = cluster_labels_tuned
    clustering_model_name = f'K-Means (k={optimal_k})'
    cluster_column_name = 'Cluster' # The column name in principal_df_tuned

# Option 2: Use Tuned Agglomerative Cluster Labels
elif 'agg_cluster_labels_tuned' in locals():
    principal_df_tuned['Cluster'] = agg_cluster_labels_tuned
    clustering_model_name = f'Agglomerative (n_clusters={optimal_n_clusters_agg})'
    cluster_column_name = 'Cluster' # The column name in principal_df_tuned

# Fallback to initial clustering results if tuned models aren't available
elif 'cluster_labels' in locals():
    principal_df_tuned['Cluster'] = cluster_labels # Using initial K-Means labels
    clustering_model_name = f'K-Means (k={k})'
    cluster_column_name = 'Cluster'
elif 'agg_cluster_labels' in locals():
     principal_df_tuned['Cluster'] = agg_cluster_labels # Using initial Agglomerative labels
     clustering_model_name = f'Agglomerative (n_clusters={n_clusters_agg})'
     cluster_column_name = 'Cluster'
else:
    print("No tuned or initial cluster labels found.")
    clustering_model_name = 'No Clustering Performed'
    cluster_column_name = None # Indicate that no cluster column exists


# --- Visualize the Tuned Principal Components with Cluster Labels ---
if cluster_column_name: # Only visualize if cluster labels were added
    # For 2 components, use scatter plot
    if tuned_n_components == 2:
        plt.figure(figsize=(10, 8))
        sns.scatterplot(x='principal component 1', y='principal component 2',
                        hue=cluster_column_name, data=principal_df_tuned,
                        palette='viridis', s=50)
        plt.title(f'Principal Components ({tuned_n_components} components) with {clustering_model_name} Clusters')
        plt.xlabel(f'Principal Component 1 ({pca_tuned.explained_variance_ratio_[0]:.2f}% variance)')
        plt.ylabel(f'Principal Component 2 ({pca_tuned.explained_variance_ratio_[1]:.2f}% variance)')
        plt.grid(True)
        plt.show()
    # For more than 2 components, you might need different visualizations or pairwise plots
    elif tuned_n_components > 2:
        print(f"Visualization for {tuned_n_components} components is not a simple 2D scatter plot.")
        print("Consider visualizing pairwise principal components or using a 3D plot if appropriate.")
        # Example: Pairwise scatter plots (requires seaborn)
        if tuned_n_components <= 5: # Avoid overwhelming plots for too many components
            sns.pairplot(principal_df_tuned, hue=cluster_column_name, palette='viridis')
            plt.suptitle(f'Pairwise Principal Components with {clustering_model_name} Clusters', y=1.02)
            plt.show()
        else:
             print("Too many components for pairwise plot. Consider selecting a subset.")
    else:
        print("Cannot visualize with less than 2 components.")